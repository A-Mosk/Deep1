import numpy as np
import matplotlib.pyplot as plt

def keep(a):
    #Dropout Regularization
    d = np.random.rand(a.shape[0],a.shape[1]) < keep_prob
    a = (a*d)/keep_prob
    return (a)
def sigma(x):
    return(1/(1+np.exp(-x)))
def tanh(x):
    return((np.exp(x)-np.exp(-x))/((np.exp(x))+np.exp(-x)))
def relu(x):
    x[x<=0] = 0
    return(x)
def dsigma(x):
    return(x * (1-x))
def dtanh(x):
    return(1 - np.power(x,2))
def drelu(x):
    x[x<=0] = 0
    x[x>0] = 1
    return(x)

alpha = 0.1
Max_epocs = 10000
keep_prob = 0.8

#Create Input data
X = np.array([0,0,0,0,0,0,0,1,0,0,1,0,0,0,1,1,0,1,0,0,0,1,0,1,0,1,1,0,0,1,1,1,1,0,0,0,1,0,0,1,1,0,1,0,1,0,1,1,1,1,0,0,1,1,0,1,1,1,1,0,1,1,1,1])
n0 = 4
m = 16
trainset = np.random.rand(1,m) < 0.6
X = np.reshape(X,(m,n0)).T

#Normalization
#meu = (1/m) * np.sum(X,0)
#ver = np.sqrt((1/m) * np.sum((X-meu)**2,0))
#X = X/ver

#Create Output data
Y_original = np.array([1,0,1,0,1,0,1,0,1,0,1,0,1,0,1,0,]).reshape(1,16)

#Create train and test data
Y = Y_original[trainset]
Y_test = Y_original[trainset == 0]
A = [np.delete(X, np.invert(trainset.reshape(16,)), axis = 1)]
A_test = np.delete(X, trainset.reshape(16,), axis = 1)

#Set Net parameters
nodes = (n0,4,2,1)
Net = [(1,'relu'),(2,'relu'),(3,'sigma')]

#Create wieghts and bias
i = 1
W = []
B = []
while i < len(Net):
    W.append(np.random.randn(nodes[i],nodes[i-1]) * np.sqrt(2/nodes[i-1]))
    B.append(np.zeros((nodes[i],1)))
    i = i + 1
W.append(np.random.randn(nodes[i],nodes[i-1]))
B.append(np.zeros((nodes[i],1)))

L = ([1])
Z = []
i = 0
while L[len(L)-1] >= 0.01 or L[len(L)-1] < L[len(L)-2] * 0.999:
    #print(i)
    if i > Max_epocs: 
        break
    
    #forward
    for layer in Net
        try:
            Z[layer[0]] = np.dot(W[layer[0]],A[layer[0]-1]) + B[layer[0]]
        except:
            Z.append(np.dot(W[layer[0]],A[layer[0]-1]) + B[layer[0]])
        if layer[1] == 'relu':
            try:
                A[layer[0]] = relu(Z[layer[0]])
            except:
                A.append(relu(Z[layer[0]]))
        elif layer[1] == 'tanh':
            try:
                A[layer[0]] = tanh(Z[layer[0]])
            except:
                A.append(tanh(Z[layer[0]]))
        elif layer [1] == 'sigma':
            try:
                A[layer[0]] = sigma(Z[layer[0]])
            except:
                A.append(sigma(Z[layer[0]]))
        else:
            print('error in function name')
            exit()
        if nodes[layer[0]-1] > 3:
            A[layer[0]] = keep(A[layer[0]])
    
    #Cost
    L.append(-(1/m)*np.sum(Y*np.log(A[len(A)])+(1-Y)*np.log(1-A[len(A)])))
    
    dA = -(Y/A[len[A]]) + (1-Y)/(1-A[len(A)])
    dZ = 
    
    #backward
    for layer in Net
        if layer[1] == 'relu':
            try:
                dZ = dA
    dZ_4 = dA * dsigma(A_4)
    dW_4 = (1/m)*np.dot(dZ_4,A_3.T)
    dB_4 = (1/m)*np.sum(dZ_4,axis = 1,keepdims = True)
    
    dZ_3 = np.dot(W_4.T,dZ_4) * drelu(A_3)
    dW_3 = (1/m)*np.dot(dZ_3,A_2.T)
    dB_3 = (1/m)*np.sum(dZ_3,axis = 1,keepdims = True)
    
    dZ_2 = np.dot(W_3.T,dZ_3) * drelu(A_2)
    dW_2 = (1/m)*np.dot(dZ_2,A_1.T)
    dB_2 = (1/m)*np.sum(dZ_2,axis = 1,keepdims = True)
    
    dZ_1 = np.dot(W_2.T,dZ_2) * drelu(A_1)
    dW_1 = (1/m)*np.dot(dZ_1,A_0.T)
    dB_1 = (1/m)*np.sum(dZ_1,axis = 1,keepdims = True)
    
    W_1 = W_1 - alpha * dW_1
    B_1 = B_1 - alpha * dB_1
    W_2 = W_2 - alpha * dW_2
    B_2 = B_2 - alpha * dB_2
    W_3 = W_3 - alpha * dW_3
    B_3 = B_3 - alpha * dB_3
    W_4 = W_4 - alpha * dW_4
    B_4 = B_4 - alpha * dB_4
    
    i = i + 1
plt.plot(L)
print('Y',Y.shape,Y)
print('A_4',A_4.shape,A_4)
print('i',i)
plt.xlabel('Rounds') 
plt.ylabel('Cost function') 
plt.show()
