import numpy as np
import matplotlib.pyplot as plt

def keep(a):
    #Dropout Regularization
    d = np.random.rand(a.shape[0],a.shape[1]) < keep_prob
    a = (a*d)/keep_prob
    return (a)
def sigma(x):
    return(1/(1+np.exp(-x)))
def tanh(x):
    return((np.exp(x)-np.exp(-x))/((np.exp(x))+np.exp(-x)))
def relu(x):
    x[x<=0] = 0
    return(x)
def dsigma(x):
    return(x * (1-x))
def dtanh(x):
    return(1 - np.power(x,2))
def drelu(x):
    x[x<=0] = 0
    x[x>0] = 1
    return(x)

alpha = 0.1
Max_epocs = 10000
keep_prob = 0.8

#Create Input data
X = np.array([0,0,0,0,0,0,0,1,0,0,1,0,0,0,1,1,0,1,0,0,0,1,0,1,0,1,1,0,0,1,1,1,1,0,0,0,1,0,0,1,1,0,1,0,1,0,1,1,1,1,0,0,1,1,0,1,1,1,1,0,1,1,1,1])
n0 = 4
m = 16
trainset = np.random.rand(1,m) < 0.6
X = np.reshape(X,(m,n0)).T

#Normalization
#meu = (1/m) * np.sum(X,0)
#ver = np.sqrt((1/m) * np.sum((X-meu)**2,0))
#X = X/ver

#Create Output data
Y_original = np.array([1,0,1,0,1,0,1,0,1,0,1,0,1,0,1,0,]).reshape(1,16)

#Create train and test data
Y = Y_original[trainset]
Y_test = Y_original[trainset == 0]
A = [np.delete(X, np.invert(trainset.reshape(16,)), axis = 1)]
A_test = np.delete(X, trainset.reshape(16,), axis = 1)

#Set Net parameters
nodes = (n0,2,1)
Net = [(0,'relu'),(1,'sigma')]

#Create wieghts and bias
i = 1
W = []
B = []
while i < len(Net):
    W.append(np.random.randn(nodes[i],nodes[i-1]) * np.sqrt(2/nodes[i-1]))
    B.append(np.zeros((nodes[i],1)))
    i += 1
W.append(np.random.randn(nodes[len(Net)],nodes[len(Net)-1]))
B.append(np.zeros((nodes[len(Net)],1)))

L = ([1])
Z = []
i = 0
while L[len(L)-1] >= 0.01 or L[len(L)-1] < L[len(L)-2] * 0.999:
    if i > Max_epocs: 
        break
    
    #forward
    for layer in Net:
        try:
            Z[layer[0]] = np.dot(W[layer[0]],A[layer[0]]) + B[layer[0]]
        except:
            Z.append(np.dot(W[layer[0]],A[layer[0]]) + B[layer[0]])
        if layer[1] == 'relu':
            try:
                A[layer[0]+1] = relu(Z[layer[0]])
            except:
                A.append(relu(Z[layer[0]]))
        elif layer[1] == 'tanh':
            try:
                A[layer[0]+1] = tanh(Z[layer[0]])
            except:
                A.append(tanh(Z[layer[0]]))
        elif layer [1] == 'sigma':
            try:
                A[layer[0]+1] = sigma(Z[layer[0]])
            except:
                A.append(sigma(Z[layer[0]]))
        else:
            print('error in function name')
        if nodes[layer[0]+1] > 3:
            A[layer[0]] = keep(A[layer[0]])
    
    #Cost
    L.append(-(1/m)*np.sum(Y*np.log(A[len(A)-1])+(1-Y)*np.log(1-A[len(A)-1])))
    
    dA = -(Y/A[len(A)-1]) + (1-Y)/(1-A[len(A)-1])
    
    dZ = [dA * dsigma(A[len(A)-1])]
    dW = [(1/m)*np.dot(dZ[0],A[len(A)-2].T)]
    W[len(W)-1] = W[len(W)-1] - alpha * dW[0]
    dB = [(1/m)*np.sum(dZ[0],axis = 1,keepdims = True)]
    B[len(B)-1] = B[len(B)-1] - alpha * dB[0]
    
    Net.reverse()
    #backward
    for layer in Net:
        if layer[0] == len(Net)-1:
            continue
        if layer[1] == 'relu':
            dZ.append(np.dot(W[layer[0]+1].T,dZ[len(dZ)-1]) * drelu(A[layer[0]+1]))
        elif layer[1] == 'tanh':
            dZ.append(np.dot(W[layer[0]+1].T,dZ[len(dZ)-1]) * dtanh(A[layer[0]+1]))
        elif layer[1] == 'sigma':
            dZ.append(np.dot(W[layer[0]+1].T,dZ[len(dZ)-1]) * dsigma(A[layer[0]+1]))
        dW.append((1/m)*np.dot(dZ[len(dZ)-1],A[layer[0]].T))
        W[layer[0]] = W[layer[0]] - alpha * dW[len(dW)-1]
        dB.append((1/m)*np.sum(dZ[len(dZ)-1],axis = 1,keepdims = True))
        B[layer[0]] = B[layer[0]] - alpha * dB[len(dB)-1]
        
    Net.reverse()
    i += 1

plt.plot(L)
print('Y',Y.shape,Y)
print('A',A[len(A)-1].shape,A[len(A)-1])
print('i',i)
plt.xlabel('Rounds') 
plt.ylabel('Cost function') 
plt.show()
